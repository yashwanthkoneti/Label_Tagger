import re
import math
from collections import Counter
from typing import List


class TextPreprocessor:

    DATE_PATTERNS = [
        r"\b\d{4}[-/]\d{1,2}[-/]\d{1,2}\b",
        r"\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b",
        r"\b\d{8}\b",
        r"\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)"
        r"[a-z]*\s+\d{1,2}(?:,)?\s+\d{4}\b",
        r"\b\d{1,2}\s+(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)"
        r"[a-z]*\s+\d{4}\b",
        r"\b\d{1,2}:\d{2}(?::\d{2})?\s?(?:am|pm)?\b",
        r"\b\d{4}\s?[qQ]\d\b",
    ]

    def __init__(self, min_doc_freq: int = 3, entropy_threshold: float = 3.5):
        self.min_doc_freq = min_doc_freq
        self.entropy_threshold = entropy_threshold
        self.doc_freq = Counter()
        self.fitted = False

    def fit(self, texts: List[str]):
        for text in texts:
            tokens = set(self._tokenize(text.lower()))
            self.doc_freq.update(tokens)
        self.fitted = True
        return self

    def transform(self, text: str) -> str:
        if not self.fitted:
            raise RuntimeError("fit() must be called before transform()")

        text = text.lower()
        text = self._normalize_dates(text)
        tokens = self._tokenize(text)

        output = []
        for token in tokens:
            if self._is_identifier(token):
                output.append("<IDENTIFIER>")
            else:
                output.append(self._simple_lemmatize(token))

        return " ".join(output)

    def fit_transform(self, texts: List[str]) -> List[str]:
        self.fit(texts)
        return [self.transform(t) for t in texts]

    def _normalize_dates(self, text: str) -> str:
        for pattern in self.DATE_PATTERNS:
            text = re.sub(pattern, "<DATE>", text)
        return text

    def _tokenize(self, text: str) -> List[str]:
        return re.findall(r"[a-z0-9_<>]+", text)

    def _is_identifier(self, token: str) -> bool:
        if len(token) < 4:
            return False
        return (
            self.doc_freq[token] < self.min_doc_freq
            and self._entropy(token) > self.entropy_threshold
        )

    def _entropy(self, token: str) -> float:
        freq = Counter(token)
        length = len(token)
        entropy = 0.0
        for count in freq.values():
            p = count / length
            entropy -= p * math.log2(p)
        return entropy

    def _simple_lemmatize(self, token: str) -> str:
        if token.endswith("ies") and len(token) > 4:
            return token[:-3] + "y"
        if token.endswith("ing") and len(token) > 5:
            return token[:-3]
        if token.endswith("ed") and len(token) > 4:
            return token[:-2]
        if token.endswith("s") and len(token) > 3:
            return token[:-1]
        return token
