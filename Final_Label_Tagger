import re
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from collections import Counter, defaultdict
from itertools import combinations
from torch.utils.data import Dataset, DataLoader

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score

class TextPreprocessor:
    DATE_PATTERNS = [
        r"\b\d{4}[-/]\d{1,2}[-/]\d{1,2}\b",
        r"\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b",
        r"\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*\s+\d{1,2}\s+\d{4}\b",
    ]

    def __init__(self, min_doc_freq=3, entropy_threshold=3.5):
        self.min_doc_freq = min_doc_freq
        self.entropy_threshold = entropy_threshold
        self.doc_freq = Counter()

    def fit(self, texts):
        for t in texts:
            self.doc_freq.update(set(self.tokenize(t.lower())))
        return self

    def transform(self, text):
        text = text.lower()
        for p in self.DATE_PATTERNS:
            text = re.sub(p, "<DATE>", text)

        tokens = self.tokenize(text)
        out = []
        for tok in tokens:
            if self.is_identifier(tok):
                out.append("<IDENTIFIER>")
            else:
                out.append(self.lemmatize(tok))
        return out

    def tokenize(self, text):
        return re.findall(r"[a-z0-9_<>]+", text)

    def is_identifier(self, tok):
        if len(tok) < 4:
            return False
        return (
            self.doc_freq[tok] < self.min_doc_freq
            and self.entropy(tok) > self.entropy_threshold
        )

    def entropy(self, tok):
        freq = Counter(tok)
        ent = 0.0
        for c in freq.values():
            p = c / len(tok)
            ent -= p * math.log2(p)
        return ent

    def lemmatize(self, tok):
        for suf in ["ing", "ed", "ies", "s"]:
            if tok.endswith(suf) and len(tok) > len(suf) + 2:
                return tok[:-len(suf)]
        return tok

class LabelGraphDiscovery:
    def __init__(self):
        self.graph = defaultdict(dict)

    def fit(self, label_sets):
        support = Counter()
        pairs = Counter()

        for labels in label_sets:
            labels = set(labels)
            for l in labels:
                support[l] += 1
            for a, b in combinations(sorted(labels), 2):
                pairs[(a, b)] += 1

        for (a, b), cnt in pairs.items():
            self.graph[a][b] = cnt / support[a]
            self.graph[b][a] = cnt / support[b]

        for l in support:
            self.graph[l][l] = 1.0

        return self

class WindowGenerator:
    def __init__(self, window_size=20, stride=10):
        self.window_size = window_size
        self.stride = stride

    def generate(self, tokens):
        return [
            tokens[i:i+self.window_size]
            for i in range(0, len(tokens), self.stride)
            if len(tokens[i:i+self.window_size]) > 0
        ]


class Vocabulary:
    def __init__(self):
        self.t2i = {"<PAD>": 0, "<UNK>": 1}

    def fit(self, docs):
        freq = Counter()
        for d in docs:
            freq.update(d)
        for t in freq:
            self.t2i[t] = len(self.t2i)

    def encode(self, tokens, max_len):
        ids = [self.t2i.get(t, 1) for t in tokens[:max_len]]
        length = len(ids)
        ids += [0] * (max_len - length)
        return ids, length


class WindowDataset(Dataset):
    def __init__(self, windows, lengths, doc_ids, label_sets):
        self.windows = torch.tensor(windows)
        self.lengths = torch.tensor(lengths)
        self.doc_ids = torch.tensor(doc_ids)
        self.label_sets = label_sets

    def __len__(self):
        return len(self.windows)

    def __getitem__(self, idx):
        return self.windows[idx], self.lengths[idx], self.doc_ids[idx], self.label_sets[idx]


class BiLSTMEncoder(nn.Module):
    def __init__(self, vocab_size, latent_dim=24):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, 64, padding_idx=0)
        self.lstm = nn.LSTM(64, 64, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(128, latent_dim)

    def forward(self, ids, lengths):
        x = self.emb(ids)
        packed = nn.utils.rnn.pack_padded_sequence(
            x, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        _, (h, _) = self.lstm(packed)
        h = torch.cat([h[-2], h[-1]], dim=1)
        return self.fc(h)

class SupConLoss(nn.Module):
    def forward(self, z, doc_ids, label_sets, label_graph):
        z = F.normalize(z, dim=1)
        sim = torch.matmul(z, z.T)

        loss = 0.0
        cnt = 0

        for i in range(len(z)):
            w = torch.zeros(len(z), device=z.device)
            for j in range(len(z)):
                if i == j:
                    continue
                if doc_ids[i] == doc_ids[j]:
                    w[j] = 1.0
                elif label_sets[i] & label_sets[j]:
                    w[j] = 0.7
            if w.sum() == 0:
                continue
            loss -= torch.log((torch.exp(sim[i]) * w).sum() / torch.exp(sim[i]).sum())
            cnt += 1

        return loss / max(cnt, 1)


def aggregate_embeddings(z, doc_ids):
    doc_map = defaultdict(list)
    for emb, d in zip(z, doc_ids):
        doc_map[int(d)].append(emb)

    return {d: torch.stack(v).mean(dim=0) for d, v in doc_map.items()}


def train_classifier(doc_embs, label_map, train_ids, test_ids):
    X_train, y_train = [], []
    X_test, y_test = [], []

    for d in train_ids:
        X_train.append(doc_embs[d].detach().cpu().numpy())
        y_train.append(label_map[d])

    for d in test_ids:
        X_test.append(doc_embs[d].detach().cpu().numpy())
        y_test.append(label_map[d])

    mlb = MultiLabelBinarizer()
    Y_train = mlb.fit_transform(y_train)
    Y_test = mlb.transform(y_test)

    clf = Pipeline([
        ("scaler", StandardScaler()),
        ("clf", OneVsRestClassifier(LogisticRegression(max_iter=2000)))
    ])

    clf.fit(X_train, Y_train)
    preds = clf.predict(X_test)

    print("Test micro-F1:", f1_score(Y_test, preds, average="micro"))

def main(df):
    texts = df["description"].fillna("").tolist()
    labels = [
        {l.strip() for l in s.split(",") if l.strip()}
        for s in df["labels"]
    ]

    train_idx, test_idx = train_test_split(
        list(range(len(texts))), test_size=0.2, random_state=42
    )

    pre = TextPreprocessor().fit([texts[i] for i in train_idx])
    tokens = [pre.transform(t) for t in texts]

    label_graph = LabelGraphDiscovery().fit([labels[i] for i in train_idx])

    wg = WindowGenerator()
    windows, lengths, doc_ids, win_labels = [], [], [], []

    for i, toks in enumerate(tokens):
        for w in wg.generate(toks):
            windows.append(w)
            doc_ids.append(i)
            win_labels.append(labels[i])

    vocab = Vocabulary()
    vocab.fit(windows)

    enc_w, enc_l = [], []
    for w in windows:
        ids, l = vocab.encode(w, 20)
        enc_w.append(ids)
        enc_l.append(l)

    ds = WindowDataset(enc_w, enc_l, doc_ids, win_labels)
    loader = DataLoader(ds, batch_size=128, shuffle=True)

    model = BiLSTMEncoder(len(vocab.t2i))
    opt = torch.optim.Adam(model.parameters(), lr=1e-3)
    loss_fn = SupConLoss()

    for _ in range(5):
        for ids, lens, d_ids, lbls in loader:
            z = model(ids, lens)
            loss = loss_fn(z, d_ids, lbls, label_graph.graph)
            opt.zero_grad()
            loss.backward()
            opt.step()

    all_z = model(torch.tensor(enc_w), torch.tensor(enc_l))
    doc_embs = aggregate_embeddings(all_z, doc_ids)

    train_classifier(doc_embs, dict(enumerate(labels)), train_idx, test_idx)



def predict_single_use_case(
    raw_text,
    text_preprocessor,
    window_generator,
    vocabulary,
    encoder,
    classifier,
    mlb,
    window_size=20,
    threshold=0.3,
    device="cpu"
):
    """
    Predict labels for a single raw text input
    """

    # -----------------------------
    # Phase 1: Preprocess
    # -----------------------------
    tokens = text_preprocessor.transform(raw_text)

    if len(tokens) == 0:
        return {
            "input_text": raw_text,
            "predictions": {},
            "note": "Empty text after preprocessing"
        }

    # -----------------------------
    # Phase 2: Windowing
    # -----------------------------
    windows = window_generator.generate(tokens)

    if len(windows) == 0:
        return {
            "input_text": raw_text,
            "predictions": {},
            "note": "No windows generated"
        }

    # -----------------------------
    # Encode windows
    # -----------------------------
    encoded_ids = []
    lengths = []

    for w in windows:
        ids, l = vocabulary.encode(w, window_size)
        encoded_ids.append(ids)
        lengths.append(l)

    input_ids = torch.tensor(encoded_ids, dtype=torch.long).to(device)
    lengths = torch.tensor(lengths, dtype=torch.long).to(device)

    # -----------------------------
    # Phase 4: Encode embeddings
    # -----------------------------
    encoder.eval()
    with torch.no_grad():
        window_embeddings = encoder(input_ids, lengths)

    # -----------------------------
    # Phase 5: Aggregate windows
    # -----------------------------
    norms = torch.norm(window_embeddings, dim=1)
    top_k = min(5, len(window_embeddings))
    top_embeddings = window_embeddings[norms.topk(top_k).indices]
    doc_embedding = top_embeddings.mean(dim=0).unsqueeze(0)

    # -----------------------------
    # Phase 5: Predict probabilities
    # -----------------------------
    probs = classifier.predict_proba(doc_embedding.cpu().numpy())[0]
    label_names = mlb.classes_

    label_probs = dict(zip(label_names, probs))

    # -----------------------------
    # Thresholding
    # -----------------------------
    selected = {
        lbl: float(p)
        for lbl, p in label_probs.items()
        if p >= threshold
    }

    # Ensure at least one label
    if not selected:
        best_label = max(label_probs, key=label_probs.get)
        selected = {best_label: float(label_probs[best_label])}

    return {
        "input_text": raw_text,
        "predictions": selected
    }
