import re
import math
from collections import Counter, defaultdict
from typing import List
from itertools import combinations


class TextPreprocessor:

    DATE_PATTERNS = [
        r"\b\d{4}[-/]\d{1,2}[-/]\d{1,2}\b",
        r"\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b",
        r"\b\d{8}\b",
        r"\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)"
        r"[a-z]*\s+\d{1,2}(?:,)?\s+\d{4}\b",
        r"\b\d{1,2}\s+(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)"
        r"[a-z]*\s+\d{4}\b",
        r"\b\d{1,2}:\d{2}(?::\d{2})?\s?(?:am|pm)?\b",
        r"\b\d{4}\s?[qQ]\d\b",
    ]

    def __init__(self, min_doc_freq: int = 3, entropy_threshold: float = 3.5):
        self.min_doc_freq = min_doc_freq
        self.entropy_threshold = entropy_threshold
        self.doc_freq = Counter()
        self.fitted = False

    def fit(self, texts: List[str]):
        for text in texts:
            tokens = set(self._tokenize(text.lower()))
            self.doc_freq.update(tokens)
        self.fitted = True
        return self

    def transform(self, text: str) -> str:
        if not self.fitted:
            raise RuntimeError("fit() must be called before transform()")

        text = text.lower()
        text = self._normalize_dates(text)
        tokens = self._tokenize(text)

        output = []
        for token in tokens:
            if self._is_identifier(token):
                output.append("<IDENTIFIER>")
            else:
                output.append(self._simple_lemmatize(token))

        return " ".join(output)

    def fit_transform(self, texts: List[str]) -> List[str]:
        self.fit(texts)
        return [self.transform(t) for t in texts]

    def _normalize_dates(self, text: str) -> str:
        for pattern in self.DATE_PATTERNS:
            text = re.sub(pattern, "<DATE>", text)
        return text

    def _tokenize(self, text: str) -> List[str]:
        return re.findall(r"[a-z0-9_<>]+", text)

    def _is_identifier(self, token: str) -> bool:
        if len(token) < 4:
            return False
        return (
            self.doc_freq[token] < self.min_doc_freq
            and self._entropy(token) > self.entropy_threshold
        )

    def _entropy(self, token: str) -> float:
        freq = Counter(token)
        length = len(token)
        entropy = 0.0
        for count in freq.values():
            p = count / length
            entropy -= p * math.log2(p)
        return entropy

    def _simple_lemmatize(self, token: str) -> str:
        if token.endswith("ies") and len(token) > 4:
            return token[:-3] + "y"
        if token.endswith("ing") and len(token) > 5:
            return token[:-3]
        if token.endswith("ed") and len(token) > 4:
            return token[:-2]
        if token.endswith("s") and len(token) > 3:
            return token[:-1]
        return token

from itertools import combinations
from collections import Counter, defaultdict


class LabelGraphDiscovery:
    """
    Phase 3: Label Graph Discovery

    Builds a weighted, directed label graph where:
    - nodes are labels
    - edges represent co-occurrence strength
    - weights encode confidence of association

    This avoids hard grouping and supports overlapping semantics naturally.
    """

    def __init__(
        self,
        min_support=0.02,
        min_confidence=0.05
    ):
        self.min_support = min_support
        self.min_confidence = min_confidence

        self.label_support = Counter()
        self.pair_support = Counter()
        self.graph = defaultdict(dict)

        self.fitted = False

    def fit(self, label_sets):
        """
        label_sets:
        Iterable of label collections (list or set) per training sample.
        TRAIN DATA ONLY.
        """

        num_samples = len(label_sets)

        for labels in label_sets:
            unique_labels = set(labels)

            for label in unique_labels:
                self.label_support[label] += 1

            for a, b in combinations(sorted(unique_labels), 2):
                self.pair_support[(a, b)] += 1

        self._build_graph(num_samples)
        self._ensure_self_loops()
        self.fitted = True
        return self

    def get_graph(self):
        if not self.fitted:
            raise RuntimeError("fit() must be called before accessing graph")
        return dict(self.graph)

    def get_neighbors(self, label):
        if not self.fitted:
            raise RuntimeError("fit() must be called before querying graph")
        return self.graph.get(label, {})

    def _build_graph(self, num_samples):
        """
        Build directed edges using asymmetric confidence.
        """
        min_count = int(self.min_support * num_samples)

        for (a, b), count in self.pair_support.items():
            if count < min_count:
                continue

            conf_ab = count / self.label_support[a]
            conf_ba = count / self.label_support[b]

            if conf_ab >= self.min_confidence:
                self.graph[a][b] = conf_ab

            if conf_ba >= self.min_confidence:
                self.graph[b][a] = conf_ba

    def _ensure_self_loops(self):
        """
        Ensure every label has at least a self-loop.
        This guarantees every label participates in contrastive positives.
        """
        for label in self.label_support:
            if label not in self.graph:
                self.graph[label] = {}

            self.graph[label][label] = 1.0
